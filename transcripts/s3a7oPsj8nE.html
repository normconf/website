
<!DOCTYPE html>
<html>

<head>
  <title>Normconf: Data ethics: the non-sexy parts -  Roy Keyes Transcript</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description"
    content="Transcript of Data ethics: the non-sexy parts -  Roy Keyes from Normconf" />
  <meta property="og:title" content="Normconf: Data ethics: the non-sexy parts -  Roy Keyes Transcript" />
  <meta property="og:url" content="https://normconf.com" />
  <meta name="og:description"
    content="Transcript of Data ethics: the non-sexy parts -  Roy Keyes from Normconf" />
  <meta property="og:image" content="https://normconf.com/images/normconf_banner_metadata_1000x500.png" />
  <meta property="og:type" content="website" />
  <meta name="twitter:image" content="https://normconf.com/images/normconf_banner_metadata_1000x500.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="normconf.com" />
  <meta name="twitter:creator" content="@normconf" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link href="fontawesome/css/all.min.css" rel="stylesheet">
  <link rel="icon" type="image/x-icon" href="images/favicon.ico">
  <style>
    body {
      background-image: url('images/background_placeholder.png');
    }

    .hero {
      background-color: None;
      color: #797979;
      font-family: sans-serif;
    }

    .center {
      display: block;
      margin-left: auto;
      margin-right: auto;
      width: 50%;
    }

    a:link {
      color: #4200be;
    }

    a:visited {
      color: #4200be;
    }

    a:active {
      color: #ef028c;
    }

    a:hover {
      color: #ef028c;
    }

    a.light_link:link {
      color: #ef028c;
    }

    a.light_link:visited {
      color: #4200be;
    }

    a.dark_link:link {
      color: #ef028c;
    }

    a.dark_link:visited {
      color: #ef028c;
    }

    a.dark_link:active {
      color: #5fffcf;
    }

    a.dark_link:hover {
      color: #5fffcf;
    }

    a.on_pink:link {
      color: #5fffcf;
    }

    a.on_pink:active {
      color: #6017af;
    }

    a.on_pink:hover {
      color: #4200be;
    }

    .navbar.is-dark .navbar-end>a.navbar-item:hover {
      background-color: #ef028c;
    }

    .navbar.is-dark .navbar-item {
      white-space: normal;
    }

    .navbar.is-dark .navbar-item.has-dropdown:hover .navbar-link {
      background-color: #ef028c;
    }

    .navbar.is-dark .navbar-brand>a.navbar-item:hover {
      background-color: #6017af;
    }

    .button.is-danger {
      background-color: #ef028c;
    }

    .button.is-primary {
      background-color: #5fffcf;
      color: #4200be;
    }

    table.speakers1 {
      font-weight: bold;
      color: #4200be;
      width: auto;
    }

    table.speakers2 {
      font-weight: bold;
      color: #6017af;
      width: auto;
    }

    table.schedule {
      font-weight: bold;
      /* width: auto; */
    }

    table.schedule td {
      text-align: center;
      padding: 0.5em;
      max-width: 30em;
      /* word-wrap: break-word; */
    }

    table.schedule tr:nth-child(odd) {
      color: #4200be;
      background-color: #00000017;
    }

    table.schedule tr:nth-child(even) {
      color: #6017af;
    }

    .message.is-primary .message-header {
      background-color: #5fffcf;
      color: #4200be;
    }
  </style>
  </head>
  <body>
  <section class="section" style="background-color: #6017af;">
    <div class="container">
    <div class="box">
      <h1 class="title">
        Data ethics: the non-sexy parts -  Roy Keyes
      </h1>
      <p class="subtitle">
        Transcript generated with <a href='https://github.com/openai/whisper'>OpenAI Whisper</a> <code>large-v2</code>.
      </p>
      </div>
      <div class="box">
        <a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=0">Before I get started on this talk about data ethics, I want to give you a little bit more </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=9">info on my background and how I came to be giving this talk today. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=18">I'm Roy. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=19">I guess that was made clear, not sort of a cheeseburger only part-time. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=26">Let's start with what I was doing before I got into the world of data science and machine </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=30">learning so that this all fits together. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=34">By training, I'm a computational physicist. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=38">I did research which centered on simulating the interaction of radiation with humans for </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=46">the purposes of cancer treatment. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=47">I also worked on some actual experiments that were investigating the use of exotic particle </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=53">beams also for cancer treatment. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=58">After grad school, I went to work in a cancer clinic doing what's called medical physics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=64">I worked in radiation therapy with big expensive radiation machines. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=69">But then I switched to data science after about a year and a half working in the cancer </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=77">clinic. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=79">I went over to the sexiest job of the 21st century, of course. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=83">This was in the early days when it was still pretty much the Wild West. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=86">Maybe you could say we're still in the early days, still the Wild West. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=90">That's a different talk. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=91">Over the past decade, I've worked at several startups that you've probably never heard </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=97">of. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=98">That's fine. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=99">I've also done a lot of independent consulting and also mostly with startups that you've </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=104">never heard of. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=105">Mostly, I tend to work with very small startups when I do my consulting. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=110">In that time, I've probably spent about half of it as a pure independent contractor and </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=114">about half of that time leading teams. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=119">If you remember the pandemic, I decided to write a book during the pandemic. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=125">That ended up being a couple books. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=127">The first book I wrote was about hiring data people based on a lot of my experience. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=133">Chris Albarn is one of the people interviewed in the book. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=136">You can put that in the plus or minus column. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=138">I'll leave it up to you. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=139">The book's called Hiring Data Scientists and Machine Learning Engineers, a Practical Guide. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=144">It is a practical guide to hiring data scientists and machine learning engineers. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=148">I will not be taking questions. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=151">The second book I wrote is a little pocket-sized overview of the main concepts in deep learning. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=156">It's called Zeph's Guide to Deep Learning. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=159">Theoretically, this is the first book in a series of small books about deep learning </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=163">and other data science and machine learning topics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=168">Let's see, what else? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=169">Somehow, I ended up becoming co-organizer of VikiConf, which this is what we're here </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=176">for. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=177">Of course, Viki forced us to change the name for some reason. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=184">Now it's NormConf. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=185">I still really don't know who Norm is, but whatever. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=189">Let's get to this talk. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=191">This talk is about ethics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=196">Before I get to the ethics stuff I'm going to talk about, first I want to talk about </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=200">the stuff I'm not going to talk about, whatever that means. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=208">There is, I would say, a whole world of data ethics topics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=216">There's a lot of names for this stuff. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=219">There's AI ethics, ML ethics, robot ethics, but for this talk, mostly I'm just going to </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=223">call everything data ethics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=227">There are lots of different areas in data ethics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=230">Some of them are pretty well known. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=231">If you're on social media or follow the news, you've probably heard of several of these </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=236">areas and that's what's on this map. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=240">There's a fair amount of overlap and the boundaries are pretty fuzzy. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=244">I didn't attempt to do any Venn diagrams or anything, but it is what it is. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=256">These are the sexy parts of data ethics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=258">They're the ones that get people's attention as well as funding and resources. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=264">Usually that's for good reason. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=266">But of these, I would say there are probably three that are the main sexiest areas at the </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=271">moment. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=276">I think I went too far with this. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=279">The first area I'll call social justice data ethics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=282">This area is concerned with the effects that current and emerging data and algorithmic </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=286">technologies are having on society, especially with regards to how these technologies further </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=292">or exacerbate issues of social justice, such as racism, sexism, gender bias, and other </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=298">issues, maybe broadly social fairness. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=302">You'll often hear terms like algorithmic bias, transparency, accountability, fairness in </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=308">this area. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=309">This area is also concerned with things like whether people who provided the training data, </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=315">they were the source of the training data, whether they were for different kinds of models, </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=319">whether they actually gave consent to do that, or whether they were fairly compensated for </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=327">providing that data. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=329">This is also an area where there's a lot of attention from big tech companies, some lawmakers, </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=334">and there's nonprofit organizations and universities that focus on this as well. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=341">The next area is existential AI risk ethics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=347">Just add ethics to everything and it becomes an area. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=350">This is an area that's concerned with the potential threats posed by human or super </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=354">human level AI. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=355">So usually when you talk about this, you talk about like the Terminator scenario, the robots </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=360">rising up. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=363">That's broadly like if humans could produce AI that could outcompete us in most ways, </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=368">but is not actually aligned with human interests, will that AI decide to just wipe us all out? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=375">This is also an area with there's nonprofit organizations, think tanks, universities that </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=381">are working in this and trying to understand what these risks actually are and how you </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=387">might mitigate these risks. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=391">The last of the big three is probably the one that's been getting the most press in </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=396">recent weeks and months. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=399">These are the generative model ethics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=402">So get your norm conf bingo cards ready because this is about stable diffusion and chat GPT. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=409">I don't know if you remember stable diffusion, that's probably faded from memory at this </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=413">point. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=414">These are the models that are trained on massive data sets of images and text, and they're </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=419">subsequently able to produce human or near human level image and text as a result of </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=425">that. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=426">And while that's very amazing, I think if you took someone from 10 years ago and showed </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=430">them this stuff, they just wouldn't believe you. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=433">But it also raises a lot of questions and some of those questions have been brought </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=437">up today in some of the talks, but is training on this data, much of which is copyrighted, </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=443">even legal? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=444">Will this put artists and writers and coders out of work? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=448">What about the bias that's in that training data? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=450">Will this clog the internet with tons of spam and other crap? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=455">Who is responsible? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=456">Who is responsible for what when you use these models? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=462">Now those are the ones I consider kind of the big three, and I guess I should say, by </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=467">the way, the first two groups, they seem to really dislike each other. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=471">If Twitter feuds are to be believed, their basic argument seems to be you over there, </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=476">you're focusing on things that don't really matter. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=480">But there's also several other relatively well-known areas of data ethics, which I'm, </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=486">let's see, trying to get there. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=489">Okay, here we go. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=491">Lethal autonomous weapon ethics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=493">So this is an interesting area. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=495">Lethal autonomous weapons, those are basically drones and robots that can pick a human target </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=499">and injure or kill that person without another human being having to make the decision to </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=504">actually pull the trigger. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=506">People working in this area are mostly focused on trying to get international treaties to </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=511">ban these types of weapons in the same way that chemical and biological weapons are banned, </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=517">which might sound very hard, but if you think about it, pretty much any government with </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=523">their resources could make chemical or biological weapons. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=526">But for the most part, they understand the consequences of actually using those in the </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=530">international stage, so they don't do that. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=532">And that's one of the, I'd say the main focus of lethal autonomous weapons ethics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=539">Maybe related to that, I guess, depending on how you look at things, is self-driving </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=542">vehicle ethics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=544">This is about cars and other vehicles that can operate without a human driver. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=550">The main probably ethical questions related to that are, for example, if a self-driving </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=556">car kills a human, who is actually liable or responsible? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=561">And also, when is a self-driving vehicle safe enough to allow in public without a human </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=568">somewhere behind the wheel? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=570">So as you can imagine, this one's pretty tightly related to regulatory efforts. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=577">Privacy ethics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=578">Privacy ethics is concerned with privacy. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=581">If you're building something, how much information should you realistically collect or do you </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=586">need to collect? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=588">If you're building a flashlight app for a phone, do you really need to know someone's </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=593">detailed location or their demographic info? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=596">What kinds of information should actually be not disallowed from being collected at </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=601">all? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=602">And obviously, this is an area where there's a lot of regulatory action and effort that's </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=606">going around that. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=611">There's also this more general thing of job displacement ethics, not just with the generative </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=617">models we were talking about a second ago. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=619">And this is a discussion that's been going on since the Industrial Revolution, if not </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=625">before, so at least 100 plus years. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=629">Part of the question now is with the AI revolution as we know it, is this time different? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=634">Are we going to suddenly put out way more people, displace them from their jobs? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=638">Some of the common proposals related to this are things like guaranteed universal basic </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=643">income or even outlying certain kinds of technologies. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=647">And like I said, this is arguments that have been going on for decades, if not centuries, </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=653">this point. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=654">Now, while there are a lot of areas of ethics all over this map, these are the areas that </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=661">this talk is not really about. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=664">In fact, I'm of the opinion that the data ethics world is actually much larger than </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=668">this. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=669">And most of what most data people encounter is not really this, even though it's what </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=676">they're most likely to hear about. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=678">Now, if we zoom out, we see that the part we were just looking at, that's the sexy part </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=686">of the world of data ethics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=687">And it's actually pretty small. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=689">On this map, all the sexy stuff is up in Northern Canada and Greenland. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=693">So you can interpret that however you wish. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=696">But instead, most of the world data ethics is the non-sexy, more mundane, everyday, ethically </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=702">challenging situations. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=703">Hopefully, you don't run into that every day, but every day in the sense of every day. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=708">But let me digress quickly about why, kind of what led me to think about this in particular. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=715">Back when I was in the cancer clinic, our main focus was on safety. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=720">We were using something very, very dangerous, which is intense levels of radiation to try </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=725">to do good. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=726">But if things went bad, people could die. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=730">So there's a lot, lot, lot of effort around safety, a lot of discussion, a lot of professional </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=736">discussion, just a lot of what you're doing is towards that. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=738">Now, my next job, I fast forward a little bit, as a full-time data scientist was essentially </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=745">delivering pizza. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=747">And well, technically, I was working on the data science stuff to support pizza delivery, </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=752">even though I did do some pizza delivery to understand how this all worked. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=755">Now, while that's a much nicer moral burden, instead of people potentially dying if things </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=763">went wrong, cold pizza is probably about the worst you could get to. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=766">But on the other hand, I still saw scenarios where there were potential ethical or whatever </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=771">kind of problems you want to imagine. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=775">Now, being on social media and whatever, I see a lot of the sexy stuff that ends up in </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=780">the headlines. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=782">And I do think that it's important and good that researchers are going towards a lot of </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=785">those questions. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=786">But, you know, that's not the kind of stuff I was seeing. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=789">So there was sort of this disconnect between what I was hearing about and what I was seeing, </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=792">which was very different from my sort of previous career. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=797">And I think that it's not really what most data people see in their daily work. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=801">Most people are not working at the scale that's affecting millions of users. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=805">Obviously, some people are. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=807">But and also, most people are not advancing the cutting edge of AI where their next thing </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=813">might be the key that brings about the robot apocalypse. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=817">Obviously, some people are, too, I guess, if that's going to happen. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=821">Instead, most people are likely to run into more mundane issues that they don't talk about </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=826">in the headlines, but that doesn't mean they're not important. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=831">So this is really the meat of the talk. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=833">A bunch of lead up to it. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=835">So we're going to look at some examples of scenarios of the non-sexy data ethics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=840">Some of these are situations that I was in or I witnessed, but most of them are stories </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=845">I've heard from professional peers and names are omitted and details are changed, of course. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=852">So we're going to zoom in and look at a few of these in different parts of the data world. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=857">So this I'm calling the exec abuses your analysis. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=860">Caveats be damned. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=863">So imagine you've been tasked with doing an analysis to find areas that the company should </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=867">focus on to improve its operations. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=869">You sort through the dirty data, you do your analysis, produce a report or a dashboard </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=874">showing how different parts of the company are doing. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=876">You're rightfully proud of this. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=878">You did a lot of good work and you present this in a meeting with a bunch of executives </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=881">and stuff. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=882">One of those execs hones in on the plot showing the error rate of some operational employees </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=887">and immediately declares that the people on this end of the distribution are the ones </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=892">that are going to get fired first. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=895">Now you realize you suddenly have several issues. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=899">This was not the premise of the analysis you did. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=902">You know, that wasn't supposed to be about that. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=905">But also, you know, you know that the anomalous performance is really actually an artifact </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=911">of the data and was not the focus of your analysis. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=914">You know that like you can't someone can't just take that data at face value and use </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=919">it to make these decisions. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=921">So the question is, what do you do? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=924">Lot of scenarios going to sound like that. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=927">Why did we split this data up again? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=929">A large international company that you have heard of, I'm sure, hires a team of consultants </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=934">also from a large international company that you've probably heard of. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=938">The department that hired these consultants didn't actually have ML experience in-house. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=945">But one of the team members thought something was a little off and asked someone they knew, </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=949">a data scientist from another department to come just take a look. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=953">The data scientists begin asking questions to the consultants and about how they achieved </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=958">high performance with their model. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=960">And it turned out that the great model performance was because the consultants had been knowingly </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=966">testing on the training set. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=969">So which of course we all know that's a no-no. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=973">But they were doing that because it looked good. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=975">ETAs, what if we just fudge them? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=979">So imagine you work in food delivery. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=983">Some of us have been there. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=984">And you've been tasked with investigating the quality of delivery time estimates and </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=988">how the quality affects revenue. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=991">While analyzing the data, you realize that if your company were to bias the ETAs lower </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=995">by just a few minutes, it could result in a tangible increase in revenue. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1000">So if you just lie a little bit, a lot more people are going to place their orders and </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1004">bring your company more money. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1006">Well, what do you do in that scenario? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1009">In the real scenario, that data scientist decided to just sort of keep this to themselves </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1013">because they didn't think it would be a good thing to be known within the company. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1016">It's too tempting. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1018">And I'll just note that this was not a company I worked at, not me, even though I was in </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1021">food delivery. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1024">This is what the boss wants. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1027">A high-up exec wants a dump of data with lots of personal or private customer info. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1032">And your manager asks you to get that data and email it to the exec. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1037">You know that this is sketchy if it's not probably against some internal policy and </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1042">maybe even against some regulatory stuff or legal stuff. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1046">But your manager insists you can't make those execs unhappy. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1049">So what do you do in that situation? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1051">How do you handle it? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1053">Let's go now to big data that wasn't so big. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1058">Your team is tasked with building models to predict the outcome of certain events. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1064">This is a marquee project of your company. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1066">It got that headline, digital transformation, AI effort, whatever it is. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1072">You simply don't have enough data to produce anything reasonable. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1076">And you can't feasibly collect more data for this specific project. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1080">Now of course, your manager and the higher-ups insist that this project needs to work regardless </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1085">of whatever your complaints are as these data people. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1088">So what do you do in this situation? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1092">Next we get to the sudden promotion. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1094">You're asked to attend a meeting with potential customers or investors and your manager says, </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1099">when I tell them that you're the principal AI scientist, you just nod. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1103">Of course, in reality, you're more like a junior data analyst and nothing like that. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1108">But so you know that this is that something is not right somehow. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1114">What do you do in that situation? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1119">Choose any color as long as it's black and made by us too. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1123">So you're asked to create a model that offers unbiased recommendations for customers. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1128">Maybe those recommendations are for your company's products or maybe they're for your competitors' </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1132">products. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1133">Of course, this is a public unbiased thing, but unofficially you're told to adjust the </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1138">bias in favor of your company's products to help the bottom line. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1142">How do you deal with a situation like this where publicly it's supposed to be fair, but </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1150">you know that it is not? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1152">Also related to that is the key value proposition. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1156">You discover that the central model based product of your startup is actually not able </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1161">to do what the marketing material claims it can do. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1165">Your company is raising money based on this claim. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1168">You've actually seem to be the only person who's really looked into this. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1172">And you know that if it's actually used in a real world scenario, that it will fail spectacularly. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1177">No one's listening to your warnings that the sky is falling. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1181">What do you do in this kind of situation? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1184">And there's a lot more. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1187">For example, you discover that the great improvement claimed by another team isn't actually an </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1193">improvement at all. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1194">No one actually bothered to measure anything, but they of course made this big announcement </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1197">or maybe they purposely didn't release the numbers. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1200">You discover that your company or your team is using a piece of software and its core </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1204">product that has an incompatible license. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1207">You know, maybe it's not actually open source or it has a license that says you need to </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1211">release the changes into the public. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1214">Or you just play fast and loose with user data like GDP, CCPA. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1219">I've never even heard of that, right? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1220">Or it's just so tempting when it's much easier to ignore those kinds of regulations. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1228">So I'm thinking about all these kinds of examples and trying to think about what are the common </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1234">themes in these kinds of scenarios. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1238">And there's a few that jumped out at me. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1240">So people fudging their numbers is one of the big scenarios. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1245">It's just so tempting in many scenarios, especially when there are not others around you who could </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1251">reasonably tell the difference. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1253">Either they don't have the technical understanding or they don't have the time to dig into it </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1258">to really be able to tell the difference between your numbers and not your numbers. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1263">Or generally, this is something called lying, but it happens a lot. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1270">Then there's the situation where someone else is trying to get you to fudge the numbers. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1275">So this is probably often due to typical unethical behavior, but also when the, I'd say the cold </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1281">hard truth of how difficult data related projects are often hits the people who are the stakeholders. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1287">And you know, they were hoping you were going to go do that data magic for them. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1293">But then when reality strikes, you know, they're just tempted to say, okay, let's kind of fudge </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1298">this. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1299">Also, often the people just want you to provide them with whatever data is going to support </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1304">their already established opinion. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1306">You know, oh, just kind of cherry pick the things that I already believe anyway. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1311">And more generally, this is often a case of power imbalances in the workplace. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1316">You know, when the big boss tells you to do something and you know, how do you deal with </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1321">that, especially when they're doing something unethical. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1325">And then finally, there's your numbers take on a life of their own. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1328">This is, I would say, this is often about nuance. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1332">This is related to some of the stuff that, that some of that Katie Bauer talked about. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1338">This is kind of the flip side of that. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1340">And some of the other stuff people have presented about clearly communicating what's going on. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1344">But I think it's a very hard one because, you know, you want to balance presenting the </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1348">detailed technically correct information with, you know, sort of some distilled actionable </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1354">insight or info or whatever that is. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1357">And it's easy to, to land too far to one side. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1361">And in this case, I'd say it's usually when you land on the side where you're presenting </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1365">things too simply and or people may willingfully ignore details to, you know, confirm their </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1372">preconceived notions. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1373">But others will see things in the data or your numbers that, that are not really there </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1379">or just whatever they want to believe. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1381">So you have to consider the possible misuses or misunderstandings and think about what </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1386">your obligations are and explaining things, you know, what, how could this go wrong? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1390">There was some talk about that earlier. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1392">And even though people don't want to hear about all those technical details, like what </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1396">is your obligation to prevent people from running with this in and doing bad stuff? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1401">Of course, this is much, much easier said than done. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1404">All right. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1406">So what do we, what do we do about all this? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1409">This isn't a TED talk, so I'm not going to offer any nice clean answer. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1412">This is NormConf, Vicki NormConf. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1416">And, but there are a few suggestions that typically come up. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1420">One is around ethics training. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1422">My impression is that there are mixed conclusions on how effective ethics training is for helping </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1428">people make real world ethical decisions or reducing incidents of like criminal or unethical </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1434">behavior. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1437">And as data people, you know, it's probably easy to imagine the difficulties in measuring </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1442">how well these kinds of things work. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1444">So I don't know if that's really so clear. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1448">Another one is like culture. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1450">How does culture affect things? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1451">My opinion is that for this stuff, that the culture is set probably by two main things </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1455">within an organization. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1456">One is leaders modeling desired behaviors. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1459">You know, you want your leaders to be the model of ethical stuff and in a way that you </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1465">can relate to. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1467">The second one is incentivizing desired behaviors and de-incentivizing non-desired behaviors. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1471">I'm assuming that those, you're desiring ethical behavior in this. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1476">How well does that actually affect things? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1479">That's also another open question, but I think this is probably an area to focus on. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1484">Regulation, of course, you know, there are certain scenarios that are currently subject </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1490">to regulatory scrutiny and probably a lot that will be soon. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1494">On the other hand, I'd say that those are mostly probably more falling on the sexy part </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1498">of the map because there are things that, you know, end up in the news or whatever. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1503">And a lot of these issues probably don't fall under that. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1506">So what are my actual conclusions? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1510">I think the world of ethics is big. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1512">It's bigger than what you hear about in the news or on Twitter, if you remember Twitter. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1517">You need to be aware of the rest of the world of data ethics. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1521">A lot of it is sort of normal job ethics, you could argue, but with the additional sort </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1525">of how to lie with statistics part thrown in, I think you should think about how you </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1530">would deal with these types of scenarios. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1533">Often the real world is very vague and frogs are slowly getting boiled. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1537">You know, often don't realize that you're in an ethics situation until you're already </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1542">in it and it's too late. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1544">So I think you need to ask yourself how, when you're in the situation, if you're going to </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1549">look back on what you did, you know, how would you judge yourself about that? </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1552">But I think it's also particularly important for early career people that may not have </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1556">encountered this stuff before, and they may not have really heard anyone talk about it. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1561">So you know, you senior level people, it's a good thing to bring up to those earlier </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1567">people. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1568">That's it. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1569">That's the end of the talk. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1570">I want to thank all the NormCom organizers, volunteers, sponsors, and donors. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1577">You can find me on Twitter and Mastodon. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1580">I have a bunch of websites. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1581">I'm doing a promo of my new book in the Zeph's Guides booth channel. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1585">So stop by if you want a chance to win a book or some schwag. </a><a href="https://www.youtube.com/watch?v=s3a7oPsj8nE&t=1591">And now I will stop sharing. </a>
      </p>
    </div>
  </section>
  </body>
</html>