
<!DOCTYPE html>
<html>

<head>
  <title>Normconf: All my machine learning problems are actually data management problems - Shreya Shankar Transcript</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description"
    content="Transcript of All my machine learning problems are actually data management problems - Shreya Shankar from Normconf" />
  <meta property="og:title" content="Normconf: All my machine learning problems are actually data management problems - Shreya Shankar Transcript" />
  <meta property="og:url" content="https://normconf.com" />
  <meta name="og:description"
    content="Transcript of All my machine learning problems are actually data management problems - Shreya Shankar from Normconf" />
  <meta property="og:image" content="https://normconf.com/images/normconf_banner_metadata_1000x500.png" />
  <meta property="og:type" content="website" />
  <meta name="twitter:image" content="https://normconf.com/images/normconf_banner_metadata_1000x500.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="normconf.com" />
  <meta name="twitter:creator" content="@normconf" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link href="fontawesome/css/all.min.css" rel="stylesheet">
  <link rel="icon" type="image/x-icon" href="images/favicon.ico">
  <style>
    body {
      background-image: url('images/background_placeholder.png');
    }

    .hero {
      background-color: None;
      color: #797979;
      font-family: sans-serif;
    }

    .center {
      display: block;
      margin-left: auto;
      margin-right: auto;
      width: 50%;
    }

    a:link {
      color: #4200be;
    }

    a:visited {
      color: #4200be;
    }

    a:active {
      color: #ef028c;
    }

    a:hover {
      color: #ef028c;
    }

    a.light_link:link {
      color: #ef028c;
    }

    a.light_link:visited {
      color: #4200be;
    }

    a.dark_link:link {
      color: #ef028c;
    }

    a.dark_link:visited {
      color: #ef028c;
    }

    a.dark_link:active {
      color: #5fffcf;
    }

    a.dark_link:hover {
      color: #5fffcf;
    }

    a.on_pink:link {
      color: #5fffcf;
    }

    a.on_pink:active {
      color: #6017af;
    }

    a.on_pink:hover {
      color: #4200be;
    }

    .navbar.is-dark .navbar-end>a.navbar-item:hover {
      background-color: #ef028c;
    }

    .navbar.is-dark .navbar-item {
      white-space: normal;
    }

    .navbar.is-dark .navbar-item.has-dropdown:hover .navbar-link {
      background-color: #ef028c;
    }

    .navbar.is-dark .navbar-brand>a.navbar-item:hover {
      background-color: #6017af;
    }

    .button.is-danger {
      background-color: #ef028c;
    }

    .button.is-primary {
      background-color: #5fffcf;
      color: #4200be;
    }

    table.speakers1 {
      font-weight: bold;
      color: #4200be;
      width: auto;
    }

    table.speakers2 {
      font-weight: bold;
      color: #6017af;
      width: auto;
    }

    table.schedule {
      font-weight: bold;
      /* width: auto; */
    }

    table.schedule td {
      text-align: center;
      padding: 0.5em;
      max-width: 30em;
      /* word-wrap: break-word; */
    }

    table.schedule tr:nth-child(odd) {
      color: #4200be;
      background-color: #00000017;
    }

    table.schedule tr:nth-child(even) {
      color: #6017af;
    }

    .message.is-primary .message-header {
      background-color: #5fffcf;
      color: #4200be;
    }
  </style>
  </head>
  <body>
  <section class="section" style="background-color: #6017af;">
    <div class="container">
    <div class="box">
      <h1 class="title">
        All my machine learning problems are actually data management problems - Shreya Shankar
      </h1>
      <p class="subtitle">
        Transcript generated with <a href='https://github.com/openai/whisper'>OpenAI Whisper</a> <code>large-v2</code>.
      </p>
      </div>
      <div class="box">
        <a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=0">My name is Shreya. I am a PhD student at the UC Berkeley Epic Club and I work on </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=7">data management for machine learning. So like very much continuing with the theme </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=12">of what we've been doing today. In previous lives I've been a machine </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=16">learning engineer and the reason that I kind of went to do a PhD is it's like a </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=22">fully funded experience to sit back and fundamentally rethink the way that we do </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=27">things. So that's why I'm here and I'm super excited to share with you what I'm </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=31">thinking about in the last couple of weeks. And this talk is brought to you by </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=37">hundreds of hours of databases, at least studying about databases, machine </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=42">learning debugging, and stable diffusion. So it's no surprise that machine </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=50">learning is kind of taking the world by storm in the way that we develop </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=56">applications across industries, across different sizes of companies, but it's </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=62">not very pretty in practice. I mean we have a whole conference track </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=67">dedicated to this, but there are a bunch of bugs that happen when you have </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=72">machine learning in production. I'll talk about two of some of my favorite. One is </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=77">you have this ML model that takes in a feature that relies on a different part </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=82">of the code base and if an engineer makes a change to that code base that </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=86">causes this kind of get status to fail, how would you know that really? And maybe </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=91">it's failing and it's failing quietly, it's returning like a negative value for </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=96">example, something that kind of doesn't put an error out there and it really </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=101">begs the question, at least the authors of this paper ask, do you know when </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=105">your data gets messed up? Are you able to do that? Who knows? And another kind of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=111">bug was when a healthcare and company is kind of monitoring patient outcomes and </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=120">they take a lot of vitals through a mobile app logging device and one of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=126">their patients kind of was just their battery on their phone was dying over </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=129">time and they stopped sending data in. So this is kind of complete opposite, it's </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=134">not that the data is changing, it's that the data is staying the same when you're </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=138">pulling the latest data, you know when you stop getting data. This seems like a </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=143">nightmare to kind of catch all these different bugs out there and I feel like </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=149">we've had this sentiment really in building and debugging ML pipelines </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=155">where it's like when we train an ML model and it works well and we feel </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=160">satisfied that it actually accomplishes something that we want, unfortunately we </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=166">kind of throw the model over the wall, if you've heard that sentiment before, and </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=170">it becomes some sort of ops problem. And what does it mean to have an ops problem </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=174">now? It means you have an engineer out there just sitting there collecting </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=179">bugs, getting super overwhelmed, and most of these ML failures really have nothing </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=185">to do with machine learning. They have something to do with some data </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=188">corruption in the pipeline, some engineering bug as I mentioned earlier, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=192">but still someone is tasked of identifying that and then </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=197">trying to fix that as quickly as possible. And kind of as Jeremy mentioned </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=201">in the previous talk, when you try to fix things very quickly you tend to use </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=205">rules, you tend to use other things off the shelf, you don't want to go and try </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=209">to retrain a model for every bug that you get, and you end up stitching </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=214">together all sorts of tools, all sorts of different rules, all sorts of different </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=218">filters, and it creates something called a pipeline jungle. I </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=224">really really love this term because it's a jungle really not just of tools, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=229">it's also a jungle of people. You know this diagram gives me anxiety, I think it </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=233">also gives Vicky anxiety, I'm not even gonna go through it, I'm just gonna put </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=237">it out there. And this diagram also gives me anxiety, it's a jungle of tools. And </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=242">when you have all of these tools, one thing very very terrible about it but </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=249">natural, is you feel like you're getting sold snake oil. At least I felt like I've </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=253">been getting sold snake oil. Last year I went to some networking event and </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=258">someone was telling me they had this new tool where all you had to </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=261">do is add decorators to your functions and then set their dependencies and set </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=266">your schedule, and it's just a few lines of code, but you can get </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=270">some production ML pipeline. And I'm sitting here thinking like okay I don't </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=274">even know if I want to productionize anything but it's a Jupyter Notebook </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=277">that I want to productionize. Another networking event that I went to is </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=281">someone's trying to tell them sell me feature stores and while they might be </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=285">useful for a lot of cases, I was very skeptical, I was very happy with my table </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=292">of features that I was just adding to on a schedule. I didn't really know why I </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=296">needed feature stores but I felt like I was being sold this a lot. And maybe that </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=301">was just me at a startup but I'm sure a lot of people have kind of similar </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=306">experiences here. But it's not just this kind of feeling of getting sold snake </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=310">oil that's a bad thing about pipeline jungles. Lots and lots of reasons why </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=316">pipeline jungles suck, you know, onboarding sucks. There are tons and </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=322">tons of bugs that come up out there. It takes forever to go and diagnose a bug </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=329">where you don't even know where it is. And at least this is my opinion, I </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=334">really think it requires some sort of PhD or equivalent of experience. You know </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=339">you work for so many years and collect all this expertise just to be able to </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=343">maintain or have hope of maintaining these pipelines in production. I can </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=348">think of, you can think of many many more. But the goal of this talk really is to </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=352">give you a data practitioner or software engineer without like a PhD in machine </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=358">learning, someone who's not training machine learning models but you're </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=362">working on ML-powered software. I want to give you maybe a new different </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=367">framework for interpreting ML bugs. And kind of spoiler alert, it has to do with </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=373">materialized views. So before we get into it, at least before we get into the new </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=380">view of ML bugs, I want to talk about how we kind of get to pipeline jungles in </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=385">the first place. I have this section called Jupiter notebook, from Jupiter </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=390">notebook to pipeline jungle. And I took this example from a paper that I wrote </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=395">earlier this year. But the first kind of step in ML is just to identify, you know, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=401">is ML even the correct pop or ML even the correct tool for you to solve this </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=405">problem? Can you actually train a model that gets good performance on some set </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=409">of data? So maybe we open up a notebook, we experiment, you know, we do EDA, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=414">exploratory data analysis. And this is, this example is also just taken from the </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=420">internet, scraped from GitHub. And we create this notebook and maybe we do </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=425">decide that, you know, this model outputs something reasonable, and we want to go </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=432">ahead and productionize it. And we have different kind of components in this </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=436">notebook workflow. And what does it mean to really productionize something like </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=442">this? So one mode of production ML, I call single use production ML. And that </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=449">is I want to take the results from this workflow and present them to someone </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=454">else or have that kind of implicitly inform business decisions. So maybe I </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=459">clean it up, get rid of my EDA, and I only include the relevant code. And then </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=465">I submit these results and I am happy I call it a day. And that's great. There's </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=470">also a different, another mode of production ML, which I'll call multiple </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=475">use production ML, which is I want to run this on a regular basis when the </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=479">underlying data is training, or sorry, underlying data is changing. I want to </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=485">run this regularly, I want it to continually provide value, and I want to </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=488">do it with some form of low latency. So I want to take the same thing. And I kind </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=493">of want to construct some directed acyclic graph, a DAG out of these </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=500">different nodes. And I don't have the tags here, but the same like data loading </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=505">tag and the same fitting the model tag. But there are also in multiple use </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=511">production ML, there are also different stages that we need to do, especially if </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=516">we're getting continually changing data, right, we need to do data cleaning or </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=520">some sort of validation. Maybe we have an inference node, quote unquote, serves a </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=526">model. And very, very quickly, you go from something like this to something </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=533">like this. And this is taken from a blog post from Uber a while back that was </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=537">motivating feature stores. And the idea here is you have this DAG of dependent </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=543">or you have this DAG of nodes that need to run in order to be able to serve real </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=548">time, or even like low latency predictions, you have your model, you </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=552">have your data preparation, and you have your inference. And maybe you start out </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=559">with something like this just to meet your like online requirements. But as </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=565">kind of ML engineers observe new bugs that come in over time, they need to </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=570">react quickly, right? So they how do you patch this whole pipeline up to prevent </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=576">against failure modes in the future. And an interview study that I did quite </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=580">recently, I love this quote. But whenever they deployed their model, it was a </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=587">chatbot like customer service kind of model. And someone would ask the model </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=591">the language model, like what time is the business open, the model will </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=595">hallucinate sometime 9am, something that like kind of looks right on the surface. </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=600">But if you look at kind of the website of that business, it's not going to be </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=604">it's making up times. So the ML engineer responsible for this, you know, if they </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=610">they filtered the output, if they detected time, they filtered the reply, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=613">they referred them to the website, rather than try to fine tune the model on every </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=618">single customer or every single business's web page. So what ends up </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=624">happening in the pipeline is you end up adding filters. And after the inference, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=630">sometimes you add filters even like before the model, if you can kind of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=633">detect that the data asked for something like time. So you can see how these kinds </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=640">of pipelines will get to something that is this like amalgamation of ML models </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=647">and filters. So now now argue that this kind of DAG is not really great. It </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=654">gives us a headache, gives us meaning ML engineers, for a lot of reasons. </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=659">Current ML pipeline DAG suck, because they for one, require low level </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=665">scheduling. So for every task in that diagram before, I need to set a schedule, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=671">like maybe I use airflow, I use some cron thing, I need to make sure that we </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=676">materialize each single one, each single tasks outputs on the schedule. Then most </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=683">of these tasks run on different schedules. And that's not like it makes </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=687">sense that it does run on different schedule, like data ingestion, for </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=691">example, runs maybe every day, but model retraining might run like every week. </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=697">Maybe the model retraining step even take several weeks. And so maybe it's run </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=701">every month. So given like compute requirements, just like organizational </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=706">requirements, also, these tasks end up running on different schedules. And then </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=711">when you have these kind of low level DAG requirements, people now have to </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=715">handle consistency on their own, right? What do you do when a task fails? What </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=720">do you do when someone changes the code? Do you go and backfill old outputs? All </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=725">of these questions now come up, and especially when you have rotating ML </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=729">engineers, right? Like that's an incredible amount of knowledge that you </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=732">need to share just to make sure that this works. It requires constant </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=736">constant babysitting and monitoring, which absolutely no one wants to do. And </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=742">now I'll argue that these DAGs kind of are the way they are because they're </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=747">quite training centric. What is training centric mean? It means that your </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=752">workflow starts with, you know, the goal of the workflow in the beginning is to </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=758">understand whether a model can achieve good performance. So everything is about </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=763">the model, be it data centric, model centric, whatever. It's about getting </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=768">good model that gets good validation set performance. And in this training </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=773">centric approach, right, recall like the Jupyter Notebook that I showed you </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=776">earlier, you do some sort of data preparation, you run experiments. And </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=782">this part is like really where you spend most of your initial time, right? Like </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=786">can you even get a bottle that will work? Is ML a good use here? And then </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=791">once you're satisfied with that, you can get the best model artifact and then </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=795">throw it over the wall, hand off this artifact for deployment. What does this </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=801">look like in the pipeline setting? It means your model or your kind of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=807">training job, your retraining job is really written first. Your predict job </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=813">is written second, your data preparation and cleaning is written third. And then </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=819">finally, all the online stuff, like the queries in itself, are an afterthought. </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=826">This is not great. So maybe I'll maybe I'll present to you an alternative view </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=830">the idealistic view of how we would want to do machine learning, the query </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=834">centric approach. So what does this look like? It means so that this is </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=840">actually a way to forget kind of how you've been taught ML in the past, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=844">this forget that training centric approach. Completely different way of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=849">thinking about using machine learning in software systems is to think from the </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=855">perspective on the query. When a new example, when a new query arrives, what </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=860">we want to do is retrieve the historical examples that were similar to that or </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=865">retrieve the historical examples of the same schema per se, fit a model to </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=871">these historical examples, maybe all of them, and then use this model on the </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=877">new example to surface the prediction and return that prediction to the end </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=881">user. So I'll keep this diagram here for a little bit. The idea here is we </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=889">want to, I know it sounds crazy, the idea of training a different model for </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=894">every query, but conceptually, like this is how we want to think about machine </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=899">learning, right? It's when you get a new data point and fit a model to your </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=903">existing data points, and then return a prediction. We can't do that. We're not </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=907">even close to this yet, right? Obviously, this is the highest latency policy you </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=912">could ever imagine. There's a huge gap between these training centric and query </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=919">centric worlds. I'm not going to argue that you should be doing this. That's </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=923">like the goal of research really is to figure out how we can move closer to </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=926">this world. But what is the gap between the training centric and query centric </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=932">worlds? In the training centric world, fitting the initial bottle is really </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=937">an experimental process. Like, can we even get a good model? What is the best </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=941">model even look like? What are the best minimal, like, what is a good set of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=944">features? All sorts of things, right? In the training centric world, the model in </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=950">itself is really an artifact. Whereas in the query centric world, the kind of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=955">training set, the store of all examples, that is mainly the artifact that we want </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=961">to manage in production over time. In training centric worlds, your tasks are </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=967">recomputed inconsistently, right? Data preparation is run differently than model </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=972">retraining. So those maybe run batch offline, different schedules. But in the </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=978">query centric world, right, once you get a new query, kind of everything, all of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=982">your data is as fresh as possible. It's clean, it's whatever. We have those kind </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=987">of guarantees. And you've seen this word consistency, consistent data all over my </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=994">slides. So this smells a lot like a data management problem, right? What does it </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=999">mean to have consistency in DAX? So now I will kind of pitch ML engineering to you </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1007">as materialized view maintenance. And don't worry too much if you don't </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1012">remember what views are, materialized views are. I'll do a very brief </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1015">refresher. So suppose you have a table, something like this. So tables, like you </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1023">are all familiar with this data structure of rows and columns stored in your </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1027">database. So a view is kind of a virtual manipulation of this data. So it's kind of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1034">formed as a result of a query, but it's not stored separately. You can query this </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1042">view as you would query a table. And when you do query the view, then the outputs of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1047">this view is then materialized or created when it's queried. So then the question </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1053">becomes, okay, like what is materialized views even mean? What is, how do we do </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1060">that? So materialized views are stored in the database. And they're computed when </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1068">you initially define that view and on base updates to the table. So every time I </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1073">add a row to the new table, then I recompute my view. There's a ton of open </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1080">problems or open questions in materialized view maintenance. So you're </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1085">wondering, well, how does this apply to ML pipeline land? Materialized views in </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1091">the literature really are just some form of derived data. So same thing with ML </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1096">pipelines, right? When I issue a query to an ML pipeline, that prediction is some </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1101">result of transformations of the data. So in the ML pipeline world, we can think </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1108">of views as kind of the training sets and the models that we are creating. And </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1116">the idea of view maintenance, which maintain these views, is this crazy DAG </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1122">that we're building to update the training sets in models as we get new </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1126">data points. And views can be kind of maintained, updated, all sorts of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1132">different ways. You can do it immediately. You can do it in batch, like deferred </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1137">view maintenance. So retraining is almost always done in a batch setting. You can, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1145">every time you need to recompute your view, materialize it on every update to a </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1149">base table. You can materialize a whole thing from scratch. So retrain the whole </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1152">model from scratch, for example. Or you can kind of write custom operators to </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1158">incrementally materialize them. So try to, this is super, super tedious, right? </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1162">Because you have to maintain some state. You have to maintain what you will do on </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1166">update. That's different from the initial, that requires custom logic. So, you know, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1170">food for thought, what is low latency, right? If you do things in batch, that's </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1175">low latency. If you redo things from scratch the whole time, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1181">that's easy to code up. So you can think about that later. But whole crux of my </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1186">argument now is that in these kind of ML DAGs, you have these inconsistent </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1191">materialized view maintenance policies. So when you're training models in </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1195">development, you're working off of like immediately materialized features, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1200">right? Like things that can make you iterate as quickly as possible. But when </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1204">we're training a production, since we're retraining on a cadence oftentimes, we </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1210">have this like, it's deferred view maintenance. Like this cadence kind of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1213">comes out of thin air. And, you know, when we're issuing online queries, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1217">we can't really, it's not the same assumption as this immediate maintenance </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1221">in development. And then when you're serving in prod, maybe some of your </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1225">features are immediately materialized, right? They're done online. Maybe you're </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1229">querying your batched features or joining with some batch features. So you have this </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1234">like hodgepodge of immediate and deferred policies. And we're using some </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1241">retrained model. So you have all these like crazy mismatch in policies, and </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1247">it's really no wonder that we get so many bugs. At least if you get anything </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1252">from this talk, that should be, this should be it. Like no wonder we get so </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1256">many bugs. It makes a lot of sense. So now, I know I have only a few minutes </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1261">left. So I'll finish in two minutes, I promise. But if you think about </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1266">recasting kind of ML bugs as view maintenance challenges, you can think </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1271">about them, some of them as view saleness problems, right? When you're doing, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1276">you're materializing outputs offline in batch, you might get trained </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1281">serves queue, right? You need to make sure that your model is retrained as kind of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1285">frequently as possible. In the interview study that I did, I love one of these </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1290">quotes, the retraining cadence was just like finger to the wind. I almost never </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1295">see like a very principled way of figuring out what the retraining cadence </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1299">is. And you get kind of these feedback delays also. So if labels are done, if </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1307">humans are in the loop, like labeling data, and labels only come in every </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1312">couple of weeks or so, right? This will make your training sets stale. Your </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1317">models will be stale, even in offline, right? So how do we kind of think about </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1323">that or reason about that? Then we also have view correctness problems, right? If </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1328">I run inference on bad quality data, if I retrain a model on bad quality data, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1333">right? Like all ensuing predictions from that model are also going to be bad. </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1338">I have a paper on that coming up soon. And data errors really compound, right, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1343">over time. If you have a data, if you have error ingestion, right, like the error </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1348">just grows as you move throughout the pipeline. So are you really, you got to </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1354">implement data validation and monitoring at every stage in your pipeline. And </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1359">then finally, kind of bugs that arise from the dev-prod gap. This is stuff </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1365">that I never thought about when I was an ML engineer. But it was my validation and </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1370">development time equivalent to kind of how I served at prod. So for this </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1374">retraining cadence, did my validation set have the same number of examples that I </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1381">would serve in production, the same representation, like same subpopulations, </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1386">etc. Was my validation set in development time sampled like a contiguous sample of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1392">production queries? This was almost never the case at the company that I </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1397">previously worked out. I almost always saw random train test splits. This is not </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1403">the same way that you would kind of monitor performance and production. And </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1407">finally, are you verifying this when you're promoting from development to </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1413">production, like in your CI? I'll skip through this almost, but this is really </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1418">just kind of tips and tricks for if you're an ML infrastructure engineer. </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1422">Validate everywhere, as I mentioned before, version data, training sets, and </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1428">code together. Make it super easy to check out old versions. If I time </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1433">traveled back to last week, can I get a view of the pipeline? Everything in the </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1437">pipeline of that week. This is super hard to do, but very important to kind of </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1441">get debugging in provenance as first class citizens. And with that, thanks so </a><a href="https://www.youtube.com/watch?v=sr3_DQ-RhTc&t=1447">much. </a>
      </p>
    </div>
  </section>
  </body>
</html>