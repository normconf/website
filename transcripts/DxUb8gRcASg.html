
<!DOCTYPE html>
<html>

<head>
  <title>Normconf: Spark horror stories from the field -  Guenia Izquierdo Transcript</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description"
    content="Transcript of Spark horror stories from the field -  Guenia Izquierdo from Normconf" />
  <meta property="og:title" content="Normconf: Spark horror stories from the field -  Guenia Izquierdo Transcript" />
  <meta property="og:url" content="https://normconf.com" />
  <meta name="og:description"
    content="Transcript of Spark horror stories from the field -  Guenia Izquierdo from Normconf" />
  <meta property="og:image" content="https://normconf.com/images/normconf_banner_metadata_1000x500.png" />
  <meta property="og:type" content="website" />
  <meta name="twitter:image" content="https://normconf.com/images/normconf_banner_metadata_1000x500.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="normconf.com" />
  <meta name="twitter:creator" content="@normconf" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link href="fontawesome/css/all.min.css" rel="stylesheet">
  <link rel="icon" type="image/x-icon" href="images/favicon.ico">
  <style>
    body {
      background-image: url('images/background_placeholder.png');
    }

    .hero {
      background-color: None;
      color: #797979;
      font-family: sans-serif;
    }

    .center {
      display: block;
      margin-left: auto;
      margin-right: auto;
      width: 50%;
    }

    a:link {
      color: #4200be;
    }

    a:visited {
      color: #4200be;
    }

    a:active {
      color: #ef028c;
    }

    a:hover {
      color: #ef028c;
    }

    a.light_link:link {
      color: #ef028c;
    }

    a.light_link:visited {
      color: #4200be;
    }

    a.dark_link:link {
      color: #ef028c;
    }

    a.dark_link:visited {
      color: #ef028c;
    }

    a.dark_link:active {
      color: #5fffcf;
    }

    a.dark_link:hover {
      color: #5fffcf;
    }

    a.on_pink:link {
      color: #5fffcf;
    }

    a.on_pink:active {
      color: #6017af;
    }

    a.on_pink:hover {
      color: #4200be;
    }

    .navbar.is-dark .navbar-end>a.navbar-item:hover {
      background-color: #ef028c;
    }

    .navbar.is-dark .navbar-item {
      white-space: normal;
    }

    .navbar.is-dark .navbar-item.has-dropdown:hover .navbar-link {
      background-color: #ef028c;
    }

    .navbar.is-dark .navbar-brand>a.navbar-item:hover {
      background-color: #6017af;
    }

    .button.is-danger {
      background-color: #ef028c;
    }

    .button.is-primary {
      background-color: #5fffcf;
      color: #4200be;
    }

    table.speakers1 {
      font-weight: bold;
      color: #4200be;
      width: auto;
    }

    table.speakers2 {
      font-weight: bold;
      color: #6017af;
      width: auto;
    }

    table.schedule {
      font-weight: bold;
      /* width: auto; */
    }

    table.schedule td {
      text-align: center;
      padding: 0.5em;
      max-width: 30em;
      /* word-wrap: break-word; */
    }

    table.schedule tr:nth-child(odd) {
      color: #4200be;
      background-color: #00000017;
    }

    table.schedule tr:nth-child(even) {
      color: #6017af;
    }

    .message.is-primary .message-header {
      background-color: #5fffcf;
      color: #4200be;
    }
  </style>
  </head>
  <body>
  <section class="section" style="background-color: #6017af;">
    <div class="container">
    <div class="box">
      <h1 class="title">
        Spark horror stories from the field -  Guenia Izquierdo
      </h1>
      <p class="subtitle">
        Transcript generated with <a href='https://github.com/openai/whisper'>OpenAI Whisper</a> <code>large-v2</code>.
      </p>
      </div>
      <div class="box">
        <a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=0">My name is Genia Izquierdo. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=2">I am a field engineering manager at Databricks, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=4">managing the SSA team. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=6">I was trained as a software engineer, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=9">but I turned to data engineering and I've really loved it. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=12">My framework of choice has been Spark </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=14">for the past seven years. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=15">So here we are today. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=18">So today I want to talk about things I've seen people do </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=22">using the Spark framework to develop </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=24">their data applications, which have horrified me. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=28">And I hope to scare you enough to prevent the world </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=31">from committing these Spark crimes again. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=35">So I will show you how I would have done this differently </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=37">and what from my very opinionated way, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=42">view is the right way. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=43">So I've grouped these stories into testing, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=46">performance and monitoring groups. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=49">So let's dig in. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=52">Okay, testing stories first. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=54">A while ago, I opened a very important repo </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=58">to make some changes needed to the utils functions </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=60">of a very important projects I had just been assigned to. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=63">And to my horror, there was no arrow next </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=68">to the Scala folder under the test folder, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=71">which means no tests. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=74">So I decided to open the utils file </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=77">and I see many functions in there. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=79">And all of them, of course, were untested. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=82">Needless to say, I flipped the table. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=85">But then in a different occasion, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=87">I encountered myself in a similar project. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=89">But this time when I opened the repo, there were tests. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=93">So I happily opened the file to see what the test coverage </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=97">looks like and I find this. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=102">What is this? </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=104">This is only testing that Spark can read data in. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=108">Come on, we can do better. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=109">There really isn't a need to test the Spark internals. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=113">You can be assured that that code has been tested thoroughly. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=116">So it is very hard to change production code </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=119">without knowing the implications of the changes </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=121">to the already existing code. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=123">So how do we make our code less spooky, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=126">our future lives easier and our products better? </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=129">Well, step one, the first step heading </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=133">in the right direction is modularize your code </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=135">with particular emphasis around reusable code, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=138">non-trivial transformations, UDFs and utility functions. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=142">Step two, we write the actual tests. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=145">At a minimum, you should write </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=147">a happy path test per function. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=149">Then if you want to go further, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=151">focus on what's important for each function </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=153">you want to test. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=155">You can start with covering common corner cases </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=157">and then checking to see where there are complex arithmetics </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=161">that could be prone to error. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=163">Are there any possibilities of getting </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=164">into a null pointer error? </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=166">If one of the conditions is not met </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=168">and you want to ensure the function returns gracefully, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=171">or if you require the output to be of a specific format </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=174">or schema, et cetera. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=175">So there are many things to consider in testing. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=180">So when you decide to dive in, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=183">and I encourage you to read more about it </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=185">and actually do dive in, there's a lot of docs online. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=188">There's a lot of information online that you can check out, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=191">but at the very least, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=193">please write those happy path tests. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=195">Okay. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=197">So next up for my second group of stories, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=201">I want to tell you some performance impacting horror stories. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=205">For these, I decided to tell you about </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=207">things I see every day that are just low hanging fruit, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=212">very normie, that can have an immediate impact </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=215">on the performance of your jobs. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=217">So keep in mind that these are all simplifications </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=221">of examples I've seen in production, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=222">and also that my recommendations can be applied </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=225">to exploratory work and analysis, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=227">but I'm more interested in helping you put this into action </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=230">on your production code. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=233">Okay, with that said, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=236">in order for you to see the horror in the next few examples, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=239">let's do a quick refresh of what Spark is and how it works. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=244">So under the hood of a Spark driver, we have a driver. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=248">The driver is the machine in which the application runs. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=251">It is responsible for three main things, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=253">maintaining information about the Spark application, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=256">responding to the user's program, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=258">analyzing, distributing, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=260">and scheduling work across the executors. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=263">And then we have the worker nodes. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=266">The worker node hosts the executor process. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=270">It has a fixed number of executors </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=271">allocated at any point in time. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=273">Each executor holds a chunk of data to be processed. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=277">This chunk is called a Spark partition. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=280">It is a collection of rows </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=281">that sits on one physical machine in the cluster. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=284">Executors are responsible for carrying out the work </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=287">assigned to the driver, assigned by the driver, sorry. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=291">And each executor is responsible for two things. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=294">One, execute your code, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=297">execute the code that has been assigned by the driver. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=300">And second, report the state of the computation </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=302">back to the driver. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=304">In each executor, we have a number of cores, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=308">which you can also think of as slots or threads. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=315">And Spark parallelizes at two levels. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=317">One is splitting the work among the executors. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=320">And the other one is the slots in each of them. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=323">So each executor has a number of slots </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=326">and each slot can be assigned a task. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=328">In this diagram, for example, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=330">some slots have been filled by a task </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=331">and some slots are open. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=334">Now let's look at how Spark executes your program. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=337">So using one of these clusters that I just mentioned, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=339">Spark processes your data by breaking up a large task </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=342">into smaller ones and distributing the work </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=344">among several machines, which are called workers. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=347">At the core of every Spark application </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=349">is the Spark driver program. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=352">The secret to Spark's performance is parallelism. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=356">Each parallelized action is referred to as a job. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=358">The driver converts your Spark application </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=361">into one or more Spark jobs. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=363">And each job is broken down into stages. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=366">Stages are created based on what operations </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=370">can be performed serially or in parallel. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=373">And not all Spark operations can happen in a single stage. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=376">So they may be divided into multiple stages. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=380">So each stage then is broken down into Spark tasks, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=384">which are then federated across each Spark executor. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=387">And each task maps to a single core </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=389">and works on a single partition of data. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=392">For example, an executor with 16 cores can have 16 tasks </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=396">running in parallel at any given point. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=399">So with that background in mind, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=401">that one was for you, Jesse, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=403">let's go back to the stories. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=405">So one day someone brings to my attention </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=408">that their job, they just deployed to production, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=410">is really slow. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=412">So I go in, look at the code, within three seconds, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=416">I tell them how to fix it, they do it, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=417">their job runs smoothly and they happily move along. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=420">So what did I see? </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=422">So I saw what I've seen so many times now </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=425">that I just tell people to look for it </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=427">without even looking at their code. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=429">So there's a few things going on in this particular version </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=432">that I created here for you. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=433">So in rows one and two, we load the data. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=438">In row five, we filtered the data </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=440">so that we only look at today's date, good. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=442">In row six, we perform a join, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=444">in row seven and eight, we group the data </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=446">and perform an aggregation, fine. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=448">In row 10, we print a count of the resulting data frame </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=451">and whoa, let's go back. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=454">We print a count of the resulting data frame </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=458">and we print that in a production job. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=462">So that's really bad, we shouldn't do that. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=465">And now why is that bad? </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=467">Let's unpack Spark a little bit more. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=469">In Spark, there are two types of operations. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=472">There's transformations and there's actions. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=474">Transformations are lazily evaluated, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=476">which means that the evaluation of your code is delayed </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=479">until that result is needed. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=481">There are two types of transformations. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=483">There's narrow transformations and wide. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=486">Narrow transformations are those that you can compute </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=490">using a single input partition. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=493">For example, think of a filter. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=495">Wide transformations usually require data to be shuffled </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=499">or moved between your worker nodes </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=501">because the operation involves partitions. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=504">For example, think of a group by. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=507">Your key might be in any of the input partitions. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=510">So then the second type of operations are actions. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=513">And these are the ones who trigger the physical evaluation </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=516">of the code written before, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=518">which is usually a set of transformations. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=521">Here at the bottom, we have some examples </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=523">of what narrow and wide transformations are </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=525">as well as actions. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=526">And there's a lot more </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=528">and you can see those in the Spark documentation. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=530">So with that, let's take a look at our code again. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=535">So we see that we have a single narrow transformation, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=538">which is the filter. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=539">And then we have two wide transformations, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=541">the join and the group by. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=543">So, and then we have two actions. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=546">We have the count, sorry, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=548">we have just single one action, which is the count. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=551">When we decide to print a count on line 10, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=556">Spark will actually create the execution plan </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=557">and your code will be run on your input data, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=560">which means that data that needs to shuffle for the join </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=562">and the group by will do so now. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=564">This might seem insignificant with a small amount of data </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=567">or during one of analysis. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=569">But when you deploy this code to production, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=572">you are adding unnecessary overhead to your job. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=574">Some scenarios where this can be particularly bad </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=576">are if you have really large data sets, of course, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=580">or if you are streaming </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=582">and doing some sort of unnecessary action </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=584">on every micro-batch, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=586">then that adds latency to your stream. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=588">So remember, when you go to prod, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=589">remove all unnecessary displays </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=591">and prints that trigger an action. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=594">So now the next are two different horror stories, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=598">but I've put them together </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=600">because the resulting anti-pattern is the same. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=602">So in the first horror story, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=604">someone was wondering why their job seemed to be stuck </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=606">in a very simple operation. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=608">And when I took a look, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=610">they were converting their data frame to pandas </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=612">in order to iterate over the data frame. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=615">This is always a red flag. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=617">This is always a red flag </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=618">because you rarely ever need to iterate over a data frame. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=622">For the second story, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=624">someone thought their job was taking a really long time </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=627">and it wasn't even supposed to be operating </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=629">on too much data. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=631">Okay, so why are these bad? </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=634">In both of these cases, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=635">all records in the data frame are sent to the driver, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=638">which defeats the purpose of having a distributed system </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=641">since you're no longer executing your code </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=642">in parallel on multiple cores and machines. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=645">So when you're writing your code, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=646">make sure you ask yourself </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=648">whether you need to use either of these approaches </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=651">or if you can find a Spark idiomatic way </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=655">of doing what you're trying to do. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=657">For example, one option for the first story </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=661">could be to rewrite your pandas code into Spark </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=664">to take full advantage of parallelization </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=666">and remove the iteration. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=669">But I understand that you don't always have the option </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=671">of moving away from pandas. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=673">I live in this world too. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=676">So if that's the case, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=677">I recommend you use the distributed version of pandas </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=680">available on Spark 3.2 and above. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=683">This will allow you to keep your code as is, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=685">but take advantage of more parallelization. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=689">I wanted to show you a small example </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=691">of the impact of all three options. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=694">So I've run a simple count on this dataset </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=697">that has over 7 million rows. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=699">So when you're running on pandas, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=702">it runs in a little bit over 40 seconds. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=706">When you use distributed pandas, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=709">it runs in 0.59 seconds. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=712">And that's pretty good since we barely had any code change. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=717">And then if you do the refactoring </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=720">and you write it in Spark, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=722">it returns in 0.12 seconds, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=725">which is much better again, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=726">but it would require some refactoring. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=729">So I'll let those numbers stay on the screen </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=732">for a second here, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=733">and I'll let you draw your own conclusions. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=739">Okay, so for the next one, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=742">I wanted to talk about collect a little bit more. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=744">Let's spot the differences between these two code snippets. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=749">So on the left, we first collect the data, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=753">and then we aggregate. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=754">On the second, we aggregate first, then we collect. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=758">So how big is the runtime difference, do you think? </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=761">Keeping in mind that we are still processing </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=763">the same data frame that contains over 7 million rows. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=767">Well, the first one took 3.72 minutes. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=772">The second one took 15.87 seconds. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=778">And it makes sense. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=779">In the first one, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=779">we brought all 7 million rows into the driver, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=783">and then we did all the computation in a single machine. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=785">Whereas in the second one, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=787">we took advantage of all those cores </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=788">available in the cluster. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=790">Then we brought the very small aggregated dataset </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=792">into the driver. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=794">So, sorry, I wrapped that one really quick. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=798">So I just wanted to let that sit for a second too, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=801">and then move on to the next story. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=803">So for this one, it's a story about UDFs. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=806">And what makes it extra sad, but horrific for me, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=810">is that I was the one who wrote it a long time ago </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=813">when I was learning Spark and thinking I was a Scala ninja. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=819">It made for a very good, bad example </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=821">of what I was trying to illustrate today. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=824">So I decided to let you roast me at a code conference, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=827">because where else, if not at a NormConf slide, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=832">would I be able to do this? </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=833">So here we go. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=835">Let's not look at the code yet. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=836">Don't get hung up on that. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=838">First, let's talk about the requirements. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=840">The requirements I received was that given a date </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=843">and an arbitrary day of the week, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=844">that denotes the start of the week, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=846">not necessarily Sunday or Monday. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=848">In this case, it was mostly Wednesdays. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=851">We needed to determine what week starting date </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=854">corresponded to the given date. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=858">And why is my code so bad? </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=862">Well, for starters, the absolutely unnecessary complexity. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=866">It makes it harder to maintain, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=868">and it makes it harder for anyone onboarding to the code, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=871">given that we can write this in a much simpler way. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=874">But no, the worst part of it is that we're using a UDF, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=879">when Spark native functions would do the work. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=882">So how can we rewrite all of this? </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=885">Well, like that. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=888">Let's look at that again. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=889">So all of that into this. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=894">Okay, so what just happened? </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=896">We used two Spark functions, dateTub and nextDay. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=900">And by doing so, we simplified the code and optimized it, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=903">because we removed the UDF. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=906">So why should you avoid UDFs when possible? </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=908">So Spark SQL and DataFrame instructions are compact </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=912">and optimized for distributing those instructions </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=914">from the driver to each executor. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=916">When we use code, we obfuscate all of that, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=919">and that code has to be serialized, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=921">sent to the executors, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=922">and then deserialized before it can be executed. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=925">So it's a black box to Spark optimizers. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=928">A friend of mine always says </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=931">that you should strive to master your tools. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=933">And if you work with Spark often, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=938">it really pays off to dive into this native functionality. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=942">It can make your code much more performant and maintainable, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=946">and ensures you don't get roasted in a future conference, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=949">at least not for your Spark code. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=953">Last, I wanna tell you about a monitoring story. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=955">Well, this one happened to me again, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=959">that once I deployed a job to production, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=962">and used the default cluster configuration, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=964">my team had settled upon for production jobs. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=967">The job didn't have any issues, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=969">but a year after having this job run flawlessly, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=973">our infrastructure team was performing an audit, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=977">and asked us to review our pipelines </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=979">to make sure we were using resources effectively. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=982">Well, let me tell you, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=983">this particular job was not, and I felt really bad </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=987">for all that year long waste. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=990">So to be honest, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=992">this is something that happens quite often. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=994">We deploy an application to production, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=996">and we monitor for instability, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=998">we get emails if it fails, we have health checks, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1001">that check that the output data looks all right, et cetera. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1004">But if the SLAs are met, health checks pass, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1007">and no crashes happen, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1009">usually we don't review production applications. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1011">So today I want to show you a quick monitoring option </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1015">that is available on Spark, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1016">and it's my go-to when I deploy, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1018">ever since that time when I deploy a new job, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1021">and want to make sure that it's configured correctly. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1023">So adjust your eyes, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1026">because the name of the tool is Ganglia. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1028">Ganglia looks like it's straight out of the 80s, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1031">but it does a great job </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1033">at giving you a first glance utilization overview </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1035">of your cluster. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1037">If you are using Databricks to deploy your Spark jobs, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1039">the recommended best practice </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1041">is that you put each job into its own job cluster. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1044">This allows for the cluster to only be up </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1047">for the time needed for the job to complete, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1049">then shut off afterwards. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1050">If you're using a shared cluster, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1052">either on Databricks or using open source Spark, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1054">you will still be able to use Ganglia, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1056">only that you won't be able to infer </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1058">the resource utilization for that particular job. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1061">But you would still be able to use Ganglia </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1064">and determine whether the overall cluster </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1066">has been provisioned correctly, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1068">and then talk to your infra team </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1069">using some of the tips I'm about to give you. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1072">So there are three main parts on this beautiful screen </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1075">that I wanna focus on. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1077">The first one is at the top right, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1079">and it's the memory utilization. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1081">So let's zoom in. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1083">So here, the red line that you see </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1085">is the total available memory for the cluster </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1087">at any given time. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1088">As you can see, this cluster auto-scaled up and down </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1091">a few times according to the usage of the cluster. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1094">That's why the red line goes up and down. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1097">The purple area is the memory actually being used. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1101">The green is the data that you have cached, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1103">and yellow is available memory. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1106">Just looking at this metric alone, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1108">if this was my cluster, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1109">this cluster seems to be doing all right. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1113">Next, we want to look at the CPU utilization </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1117">at the bottom left. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1118">So let's zoom in. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1120">Here we see that the percentage, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1122">sorry, in this graph, we see the percentage, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1125">what percentage of the total available CPU </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1128">was in use at any given time. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1130">As you can see, the most this cluster was used </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1133">was around Wednesday at 1 a.m. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1137">That was probably me running the code to present today. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1140">And the CPU utilization was around 70%. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1144">So now I wouldn't change the configuration right away. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1147">I would probably give it a few other days </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1150">or a few other runs to collect more data </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1152">and then decide how much I can downsize this cluster by, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1156">at least CPU wise, because as you can see, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1158">there are short bursts of utilization, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1161">but overall the cluster CPUs is fairly underutilized. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1166">So last but not least, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1168">we want to look at the cluster network performance. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1171">This is a pretty simple one. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1172">It just tells us how much data we're bringing to the cluster </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1175">and how much data we're writing out </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1177">and how long those operations are taking. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1180">So the green line is how much data we've read into the cluster. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1183">The blue line is how much data we've written out </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1187">and the horizontal length, I'm going to call it. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1194">For example, if you saw a green line </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1196">or blue line stretch horizontally for a particular point, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1199">that's how long it took to move the data in or out. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1202">So this is pretty much it with this one. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1205">The main thing to watch out for are those sustained peaks, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1210">which will indicate you need to provision a cluster </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1214">with more network bandwidth, </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1216">which you or your infrastructure team can do </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1219">by choosing a different VM type. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1222">So that is all I have for today. </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1227">Thank you all for checking out my talk </a><a href="https://www.youtube.com/watch?v=DxUb8gRcASg&t=1229">and enjoy the rest of the conference. </a>
      </p>
    </div>
  </section>
  </body>
</html>